#!/usr/bin/Rscript

# Este script compara a cobertura produzida pelos algoritmos de descarte

replications <- 100

sumCoverageDN <- matrix(ncol=4,nrow=replications)
sumCoverageDR <- matrix(ncol=4,nrow=replications)
sumCoverageDL <- matrix(ncol=4,nrow=replications)
sumCoverageDF <- matrix(ncol=4,nrow=replications)
sumCoverageOGK <- matrix(ncol=4,nrow=replications)

redCol <- 0 # Coluna da matriz com respectivo percentual de reducao

#---------------------------------------------------------------------

# Valores do campo a ser monitorado.
maxX = 100
maxY = 100
 
x<-1:maxX
y<-1:maxY

# Parâmetros para a geração dos dados e distribuição dos sensores.
kappa <- 0.5
phi <- 35
mean <- 25
variance <- 64

#---------------------------------------------------------------------


	for(qtdSamples in c(500,1000,1500,2000)) { #

		redCol <- redCol+1

		for(r in 1:replications) {

		#---------------------------------------------------------------------

		n_sensores <- 100
		inhibit <- 4

		if(qtdSamples < 1 || qtdSamples > (maxX*maxY))
			stop("Param. reducao invalido")

			source("headers/sensors.R")
			source("headers/algorithms.R")
			source("headers/auxiliarFunctions.R")

		#---------------------------------------------------------------------

			## Start simulation
			set.seed(10000*kappa + 10*phi + r) # Semente ser unica para cada processo

			print(paste("Replicação ",r))
			print(paste("Percentual de reduçao: ", qtdSamples, "%"))

			print(paste("Numero de sensores: ", n_sensores))
		    
			# Gerando campo a ser sensoriado.
			campo = mygrf(kappa=kappa,phi=phi,mean=mean,var=variance,nugget=0)

			# Gerando sensores
		  	sensores = rSSI(n = n_sensores, r = inhibit, win = square(maxX), giveup = 10^5)
			print("SSI: OK!")

			route <- muleWalk(sensores,10)
			print("ROUTE: OK!")

			# Apenas excluindo o processamento inicial.    
			# Atribuindo dados a cada sensor pontualmente.
		  	dados = readData(sensores,campo)
		  	geoDados = geo(sensores,dados)
			geoDados$coords = trunc(geoDados$coords)

			# Atribuindo dados a cada sensor considerando a célula de voronoi.
		  	hashDados = readDataVoronoi(sensores,campo)
			print("VORONOI: OK!")

		###########################################################################################################

			# Redução

			bufferSize = 0

			hashDadosSampled = hash()

			for(s in 1:n_sensores) {
				currentNode = route[s] #currentNode: double convertido p/ string
				bufferSize = bufferSize + (length(hashDados[[currentNode]]$data))*qtdSamples/100 
				if(bufferSize < 1)
					bufferSize = 1;
			}

			aux <- 0
		    	for(s in 1:n_sensores) {
				sampleSize = (length(hashDados[[as.character(s)]]$data))*qtdSamples/100; 
				# Corresponde ao tamanho dividido pelo percentual de reducao

				aux <- aux + sampleSize

				if(sampleSize < 1)
					sampleSize = 1

		      		hashDadosSampled[s] = as.geodata(sampleCentral(hashDados[[as.character(s)]], sampleSize))
		    	}

			print(paste("Buffer Size:", bufferSize))

			for(j in 1:5) { #j==1 -> OGK

				if(j==2) { # DL
					clear(hashDadosSampled)
		    			hashDadosSampled <- DropLast(hashDados, bufferSize, route)
				} else if(j==3) { # DF
					clear(hashDadosSampled)
		    			hashDadosSampled <- DropFirst(hashDados, bufferSize, route)
				} else if(j==4) { # DR
					clear(hashDadosSampled)
		    			hashDadosSampled <- DropRandom(hashDados, bufferSize, route)
				} else if(j==5) { # BNDP
					clear(hashDadosSampled)
		    			hashDadosSampled <- BNDP(hashDados, bufferSize, sensores, route, ceiling(n_sensores/5))
				}


			    # Contagem de quantos dados foram amostrados
			    	listNamesHash <- names(hashDadosSampled)
				streamLength <- length(listNamesHash)
			    	count = 0

			    	for(s in 1:streamLength){
			      		geoTemp = hashDadosSampled[[listNamesHash[s]]]
			      		count = count + length(geoTemp$data)
			    	}

			    # Separação dos dados para gerar figura e para obter o kriging
			    	dataToKrig = matrix(nrow=count, ncol = 3) # Dados para o krig
			    	dadosSampled = matrix(nrow = maxY, ncol = maxX) # Dados para a figura
			    	count = 1

			    	for(s in 1:streamLength){
			      		geoTemp = hashDadosSampled[[listNamesHash[s]]]
			      		for(i in 1:length(geoTemp$data)){
						dataToKrig[count,1] <- x_ <- geoTemp$coords[i,1]
						dataToKrig[count,2] <- y_ <- geoTemp$coords[i,2]
						dataToKrig[count,3] <- dadosSampled[x_,y_] <- geoTemp$data[i]
						count = count + 1
			      		}
			    	}

				if(j==1) { # OGK
					sumCoverageOGK[r,redCol] <- sum(coverage(dadosSampled))
					print("Sample Central")
				} else if(j==2) { # DL
					sumCoverageDL[r,redCol] <- sum(coverage(dadosSampled))
					print("Drop Last")
				} else if(j==3) { # DF
					sumCoverageDF[r,redCol] <- sum(coverage(dadosSampled))
					print("Drop First")
				} else if(j==4) { # DR
					sumCoverageDR[r,redCol] <- sum(coverage(dadosSampled))
					print("Drop Random")
				} else if(j==5) { # BNDP
					sumCoverageDN[r,redCol] <- sum(coverage(dadosSampled))
					print("Drop Neighbors")
					print("------------------------------------")
				}

			}
		}
	}

write.table(sumCoverageOGK, "resultados-cobertura/coverageOGK.dat")
write.table(sumCoverageDL, "resultados-cobertura/coverageDL.dat")
write.table(sumCoverageDF, "resultados-cobertura/coverageDF.dat")
write.table(sumCoverageDR, "resultados-cobertura/coverageDR.dat")
write.table(sumCoverageDN, "resultados-cobertura/coverageDN.dat")
